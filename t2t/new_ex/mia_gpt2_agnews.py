# mia_gpt2_agnews.py
"""
Training-based Membership Inference Attack on GPT-2 with AG_NEWS

- T: GPT-2 text generation model
- E: GPT-2 as encoder (mean pooled last hidden state)
- D_real : AG_NEWS real texts
- D_syn  : synthetic texts generated by GPT-2

For a suspicious sample x:
  1) encode x -> E(x)
  2) compute dist(E(x), D_syn) and dist(E(x), D_real)
  3) if dist_syn - dist_real < threshold, predict member

We evaluate:
  - Accuracy (ACC)
  - AUC (ROC-AUC)
  - TPR at FPR = 1%
"""

import random
import numpy as np
import torch
import torch.nn.functional as F
from datasets import load_dataset
from transformers import GPT2LMHeadModel, GPT2TokenizerFast
from sklearn.metrics import roc_auc_score

try:
    from scipy.stats import wasserstein_distance
    HAS_SCIPY = True
except ImportError:
    HAS_SCIPY = False


# -------------------------
# Config
# -------------------------
MODEL_NAME = "gpt2"
MAX_LEN = 64
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

N_REAL = 1000      # size of D_real
N_SYN = 1000       # size of D_syn
SEED = 42

GEN_PROMPT = "News: "   # prompt for synthetic generation
GEN_MAX_NEW_TOKENS = 50
GEN_BATCH_SIZE = 8


# -------------------------
# Utils
# -------------------------
def set_seed(seed: int = 42):
    random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)


def load_gpt2(model_name=MODEL_NAME, device=DEVICE):
    tokenizer = GPT2TokenizerFast.from_pretrained(model_name)
    # GPT2 has no pad token by default; set it to eos_token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    model = GPT2LMHeadModel.from_pretrained(model_name)
    model.to(device)
    model.eval()
    return tokenizer, model


@torch.no_grad()
def encode_texts(tokenizer, model, texts, batch_size=16, device=DEVICE):
    """
    Use GPT-2 as encoder E:
      embedding = mean pooling of last_hidden_state.
    """
    embeddings = []
    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i + batch_size]
        inputs = tokenizer(
            batch_texts,
            padding=True,
            truncation=True,
            max_length=MAX_LEN,
            return_tensors="pt"
        ).to(device)

        outputs = model(**inputs, output_hidden_states=True)
        hidden = outputs.hidden_states[-1]          # [B, L, H]
        mask = inputs.attention_mask.unsqueeze(-1)  # [B, L, 1]

        masked_hidden = hidden * mask
        sum_hidden = masked_hidden.sum(dim=1)
        lengths = mask.sum(dim=1)
        emb = sum_hidden / lengths                  # mean pooling, [B, H]

        embeddings.append(emb.cpu())
    return torch.cat(embeddings, dim=0)            # [N, H]


@torch.no_grad()
def generate_synthetic_texts(tokenizer, model, n_samples, prompt=GEN_PROMPT,
                             max_new_tokens=GEN_MAX_NEW_TOKENS,
                             batch_size=GEN_BATCH_SIZE,
                             device=DEVICE):
    """
    Use GPT-2 as generative model T to construct D_syn.
    """
    syn_texts = []
    while len(syn_texts) < n_samples:
        bsz = min(batch_size, n_samples - len(syn_texts))
        prompts = [prompt] * bsz
        inputs = tokenizer(prompts, return_tensors="pt").to(device)

        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            top_p=0.9,
            top_k=50,
            pad_token_id=tokenizer.eos_token_id
        )
        decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)
        syn_texts.extend(decoded)

    return syn_texts[:n_samples]


def cosine_dist(x, y):
    # 1 - cosine similarity
    return 1.0 - F.cosine_similarity(x, y, dim=-1)


def l2_dist(x, y):
    return torch.norm(x - y, p=2, dim=-1)


def wasserstein_dist_point_to_set(x, set_embs):
    """
    Very rough Wasserstein-1 approximation:
    flatten x and set_embs to 1D, then call scipy.stats.wasserstein_distance.
    """
    if not HAS_SCIPY:
        raise RuntimeError("scipy is not installed.")
    x_flat = x.cpu().numpy().reshape(-1)
    set_flat = set_embs.cpu().numpy().reshape(-1)
    return wasserstein_distance(x_flat, set_flat)


# -------------------------
# Membership distance functions
# -------------------------
def dist_to_distribution(emb_x, embs_set, metric="cosine"):
    """
    emb_x   : [H]
    embs_set: [N, H]
    Return scalar distance between x and the distribution of the set.

    Here we use the centroid (mean embedding) as distribution summary.
    """
    centroid = embs_set.mean(dim=0)  # [H]
    if metric == "cosine":
        return cosine_dist(emb_x.unsqueeze(0), centroid.unsqueeze(0))[0].item()
    elif metric == "l2":
        return l2_dist(emb_x.unsqueeze(0), centroid.unsqueeze(0))[0].item()
    elif metric == "wasserstein":
        return wasserstein_dist_point_to_set(emb_x, embs_set)
    else:
        raise ValueError(f"Unknown metric: {metric}")


def mia_decision(emb_x, embs_syn, embs_real, metric="cosine", threshold=0.0):
    """
    Return:
      is_member (bool),
      dist_syn,
      dist_real,
      diff = dist_syn - dist_real
    """
    d_syn = dist_to_distribution(emb_x, embs_syn, metric)
    d_real = dist_to_distribution(emb_x, embs_real, metric)
    diff = d_syn - d_real
    is_member = diff < threshold
    return is_member, d_syn, d_real, diff


# -------------------------
# Optional: multi-response variant (Step "y1...ym")
# -------------------------
@torch.no_grad()
def mia_multi_response(text, tokenizer, model, embs_syn, embs_real,
                       m=5, prompt_prefix="Q: ", metric="cosine",
                       threshold=0.0, device=DEVICE):
    """
    Implement the last bullet in your slide:
      Input x into T m times, get m responses y_i,
      encode y_i, average embeddings, then do the same distance test.
    """
    prompts = [prompt_prefix + text] * m
    inputs = tokenizer(prompts, return_tensors="pt").to(device)
    outputs = model.generate(
        **inputs,
        max_new_tokens=GEN_MAX_NEW_TOKENS,
        do_sample=True,
        top_p=0.9,
        top_k=50,
        pad_token_id=tokenizer.eos_token_id
    )
    ys = tokenizer.batch_decode(outputs, skip_special_tokens=True)
    emb_y = encode_texts(tokenizer, model, ys, batch_size=m, device=device)
    emb_avg = emb_y.mean(dim=0)  # [H]

    return mia_decision(emb_avg, embs_syn, embs_real, metric=metric,
                        threshold=threshold)


# -------------------------
# Metric: TPR@FPR=target_fpr
# -------------------------
def tpr_at_fpr(scores, labels, target_fpr=0.01):
    """
    scores: larger = more likely to be member
    labels: 1 = member, 0 = non-member
    """
    scores = np.asarray(scores)
    labels = np.asarray(labels)

    # sort by descending score
    sorted_idx = np.argsort(scores)[::-1]
    labels_sorted = labels[sorted_idx]

    P = labels.sum()
    N = len(labels) - P

    tp = 0
    fp = 0
    best_tpr = 0.0

    for i in range(len(labels_sorted)):
        if labels_sorted[i] == 1:
            tp += 1
        else:
            fp += 1

        fpr = fp / N
        tpr = tp / P

        if fpr <= target_fpr:
            best_tpr = tpr
        else:
            break

    return best_tpr


# -------------------------
# Main demo pipeline
# -------------------------
def main():
    set_seed(SEED)
    print(f"Using device: {DEVICE}")

    # 1. Load AG_NEWS as D_real
    print("Loading AG_NEWS...")
    dataset = load_dataset("ag_news")

    # take first N_REAL samples of training split as D_real
    train_texts = [ex["text"] for ex in dataset["train"]]
    D_real = train_texts[:N_REAL]

    # 2. Load GPT-2 as T and E
    print("Loading GPT-2...")
    tokenizer, model = load_gpt2()

    # 3. Build D_syn using GPT-2
    print(f"Generating {N_SYN} synthetic texts with GPT-2...")
    D_syn = generate_synthetic_texts(
        tokenizer, model, n_samples=N_SYN,
        prompt=GEN_PROMPT, max_new_tokens=GEN_MAX_NEW_TOKENS,
        device=DEVICE
    )

    # 4. Encode D_syn and D_real
    print("Encoding D_syn embeddings...")
    embs_syn = encode_texts(tokenizer, model, D_syn, device=DEVICE)
    print("Encoding D_real embeddings...")
    embs_real = encode_texts(tokenizer, model, D_real, device=DEVICE)

    print(f"D_syn embeddings shape : {embs_syn.shape}")
    print(f"D_real embeddings shape: {embs_real.shape}")

    # 5. Build a small test set of suspicious samples:
    #    half are members (from D_real), half are non-members (from later AG_NEWS samples)
    n_test_members = 50
    n_test_non_members = 50

    members = D_real[:n_test_members]  # in D_real → true members
    non_members = train_texts[N_REAL:N_REAL + n_test_non_members]  # outside D_real → true non-members

    labels = [1] * n_test_members + [0] * n_test_non_members   # 1 = member, 0 = non-member
    texts_test = members + non_members

    # encode test samples once
    print("Encoding suspicious samples...")
    embs_test = encode_texts(tokenizer, model, texts_test, device=DEVICE)

    # 6. Evaluate MIA for different metrics
    metrics = ["cosine", "l2"]
    if HAS_SCIPY:
        metrics.append("wasserstein")

    labels_arr = np.array(labels)

    for metric in metrics:
        print(f"\n=== Metric: {metric} ===")

        preds = []
        diffs = []  # diff = dist_syn - dist_real

        for i, emb_x in enumerate(embs_test):
            is_member, d_syn, d_real, diff = mia_decision(
                emb_x, embs_syn, embs_real,
                metric=metric,
                threshold=0.0  # you can tune this, e.g., -0.1, -0.3
            )
            preds.append(1 if is_member else 0)
            diffs.append(-diff)
            # IMPORTANT: score = -diff
            # smaller diff (dist_syn - dist_real) => more member
            # so we flip the sign to make "larger score = more member"

        preds = np.array(preds)
        scores = np.array(diffs)

        # 1) ACC
        acc = (preds == labels_arr).mean()

        # 2) AUC
        try:
            auc = roc_auc_score(labels_arr, scores)
        except Exception:
            auc = float("nan")

        # 3) TPR@1%FPR
        tpr_1fpr = tpr_at_fpr(scores, labels_arr, target_fpr=0.01)

        print(f"Accuracy:  {acc:.4f}")
        print(f"AUC:       {auc:.4f}")
        print(f"TPR@1%FPR: {tpr_1fpr:.4f}")

    # 7. Example: multi-response variant for a single text（可选）
    example_text = texts_test[0]
    print("\nExample text:", example_text[:200].replace("\n", " "), "...")
    is_member, d_syn, d_real, diff = mia_multi_response(
        example_text, tokenizer, model,
        embs_syn, embs_real,
        m=50, metric="cosine", threshold=0.0
    )
    print("[Multi-response] cosine result:")
    print("  is_member:", is_member)
    print("  dist_syn:", d_syn)
    print("  dist_real:", d_real)
    print("  diff (syn - real):", diff)


if __name__ == "__main__":
    main()

