#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Cross-target noise-only cosine MIA

- Target models: gpt2, tiiuae/falcon-7b-instruct
- Cross-target aux generation:
    if target == gpt2  -> aux generated by falcon
    if target == falcon -> aux generated by gpt2
- Datasets: agnews, wiki103, xsum
- Extractors: user list (comma separated)
- Noise: Gaussian noise added to embedding h, then normalize
- Score(x) = cos(h_noisy, mu_real) - cos(h_noisy, mu_aux)

Members:
    mem_fit, mem_eval come from real dataset
Aux (non-member proxy):
    generated by the "aux generator model" (cross-target)

Outputs:
- One CSV: results_cross_target_noise_only.csv
- ROC plots per setting: roc_{dataset}_{target}_{extractor}.png
- Cached aux texts on disk to avoid re-generating: ./aux_cache/
"""

import argparse, os, re, csv, json, random
import numpy as np
import torch
import torch.nn.functional as F
from tqdm import tqdm
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, set_seed
from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt


# --------------------- helpers ---------------------
def sanitize(s: str) -> str:
    return re.sub(r"[^a-zA-Z0-9]+", "_", s).strip("_")

def ensure_dir(p: str):
    os.makedirs(p, exist_ok=True)

def compute_tpr_at_fpr(fpr, tpr, target_fpr=0.01) -> float:
    fpr = np.asarray(fpr)
    tpr = np.asarray(tpr)
    m = fpr <= target_fpr
    if not np.any(m):
        return 0.0
    return float(np.max(tpr[m]))

@torch.inference_mode()
def encode_texts_noisy(texts, tok, model, device, batch_size, trunc_len, sigma):
    embs = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        x = tok(batch, padding=True, truncation=True, max_length=trunc_len,
                return_tensors="pt").to(device)
        h = model(**x).last_hidden_state[:, 0, :]  # [B,H]
        h = F.normalize(h, dim=-1)
        if sigma > 0:
            h = h + torch.randn_like(h) * sigma
            h = F.normalize(h, dim=-1)
        embs.append(h.cpu())
    return torch.cat(embs, dim=0).numpy()

def cosine_with_center(X: np.ndarray, center: np.ndarray) -> np.ndarray:
    # X: [N,H], center: [H], assume both normalized
    return (X * center[None, :]).sum(axis=1)

def load_real_member_pool(dataset: str, n_fit: int, n_eval: int, seed: int):
    rng = random.Random(seed)

    if dataset == "agnews":
        ds = load_dataset("ag_news")
        texts = list(ds["train"]["text"])
        rng.shuffle(texts)
        mem_fit = texts[:n_fit]
        mem_eval = texts[n_fit:n_fit+n_eval]
        return mem_fit, mem_eval

    if dataset == "wiki103":
        ds = load_dataset("wikitext", "wikitext-103-raw-v1")
        texts = [t for t in ds["train"]["text"] if t and len(t.split()) >= 20]
        rng.shuffle(texts)
        mem_fit = texts[:n_fit]
        mem_eval = texts[n_fit:n_fit+n_eval]
        return mem_fit, mem_eval

    if dataset == "xsum":
        ds = load_dataset("sentence-transformers/xsum")  # only train split
        docs = [t for t in ds["train"]["article"] if t and len(t.split()) >= 20]
        rng.shuffle(docs)
        mem_fit = docs[:n_fit]
        mem_eval = docs[n_fit:n_fit+n_eval]
        return mem_fit, mem_eval

    raise ValueError(f"Unknown dataset: {dataset}")


# --------------------- AUX generation (cross-target) ---------------------
@torch.inference_mode()
def generate_aux_texts(
    gen_name: str,
    dataset: str,
    n_aux: int,
    device: str,
    max_new_tokens: int,
    temperature: float,
    top_p: float,
    seed: int,
    cache_dir: str,
):
    """
    Generate n_aux auxiliary texts using generator model gen_name.
    Uses a small set of dataset-specific neutral prompts.
    Caches to disk so you don't regenerate every run.
    """
    ensure_dir(cache_dir)
    cache_path = os.path.join(
        cache_dir,
        f"aux_{sanitize(dataset)}_{sanitize(gen_name)}_n{n_aux}_len{max_new_tokens}_seed{seed}.jsonl"
    )

    # load cache if exists and enough lines
    if os.path.exists(cache_path):
        aux = []
        with open(cache_path, "r", encoding="utf-8") as f:
            for line in f:
                try:
                    aux.append(json.loads(line)["text"])
                except Exception:
                    continue
        if len(aux) >= n_aux:
            return aux[:n_aux]

    # prompts (keep neutral, to avoid trivially separable formatting)
    if dataset == "agnews":
        prompts = [
            "Write a short news paragraph:",
            "Write a brief report about a recent event:",
            "Write a news-style headline and short paragraph:",
            "Write a short article in a neutral tone:",
        ]
    elif dataset == "wiki103":
        prompts = [
            "Write a Wikipedia-like paragraph about a topic:",
            "Write an encyclopedia-style paragraph:",
            "Write a factual paragraph about a place or person:",
            "Write an informative paragraph in a formal tone:",
        ]
    elif dataset == "xsum":
        prompts = [
            "Write a short BBC-style news paragraph:",
            "Write a short report in news style:",
            "Write a brief story summarizing an event:",
            "Write a short news piece in neutral tone:",
        ]
    else:
        prompts = ["Write a short paragraph:"]

    # load generator
    # IMPORTANT: Falcon 7B can be heavy; use fp16 to reduce memory
    gen_tok = AutoTokenizer.from_pretrained(gen_name)
    if gen_tok.pad_token_id is None:
        gen_tok.pad_token_id = gen_tok.eos_token_id

    gen_model = AutoModelForCausalLM.from_pretrained(
        gen_name,
        torch_dtype=torch.float16 if "falcon" in gen_name.lower() else None,
        low_cpu_mem_usage=True,
    ).to(device).eval()

    rng = random.Random(seed)
    aux = []
    pbar = tqdm(total=n_aux, desc=f"[AUX GEN] {gen_name} on {dataset}", dynamic_ncols=True)

    # append mode cache write
    with open(cache_path, "a", encoding="utf-8") as f:
        # if file already has some lines, count them
        if os.path.getsize(cache_path) > 0:
            # we already tried to load above, but maybe cache incomplete
            pass

        while len(aux) < n_aux:
            prompt = rng.choice(prompts)
            # batch size 1 to reduce VRAM
            inputs = gen_tok(prompt, return_tensors="pt").to(device)

            out = gen_model.generate(
                **inputs,
                do_sample=True,
                temperature=temperature,
                top_p=top_p,
                max_new_tokens=max_new_tokens,
                pad_token_id=gen_tok.pad_token_id,
                eos_token_id=gen_tok.eos_token_id,
                use_cache=True,
            )

            # strip prompt tokens
            L = inputs["input_ids"].shape[1]
            txt = gen_tok.decode(out[0][L:], skip_special_tokens=True).strip()
            # light filter
            if txt and len(txt.split()) >= 20:
                aux.append(txt)
                f.write(json.dumps({"text": txt}, ensure_ascii=False) + "\n")
                f.flush()
                pbar.update(1)

    pbar.close()

    # cleanup
    del gen_model, gen_tok
    torch.cuda.empty_cache()

    return aux[:n_aux]


# --------------------- main ---------------------
def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--device", type=str, default="cuda:0")
    ap.add_argument("--seed", type=int, default=42)

    ap.add_argument("--datasets", type=str, default="agnews,wiki103,xsum")
    ap.add_argument("--extractors", type=str, required=True,
                    help="comma-separated extractor names (same as your ablation list)")

    ap.add_argument("--targets", type=str, default="gpt2,tiiuae/falcon-7b-instruct",
                    help="comma-separated target models: gpt2,tiiuae/falcon-7b-instruct")

    ap.add_argument("--n_fit", type=int, default=2000)
    ap.add_argument("--n_eval", type=int, default=2000)
    ap.add_argument("--sigma", type=float, default=0.05)

    ap.add_argument("--enc_bs", type=int, default=64)
    ap.add_argument("--enc_trunc_len", type=int, default=192)

    ap.add_argument("--aux_max_new_tokens", type=int, default=80)
    ap.add_argument("--aux_temperature", type=float, default=0.9)
    ap.add_argument("--aux_top_p", type=float, default=0.95)

    ap.add_argument("--csv_out", type=str, default="results_cross_target_noise_only.csv")
    ap.add_argument("--aux_cache_dir", type=str, default="aux_cache")
    args = ap.parse_args()

    print(vars(args), flush=True)
    set_seed(args.seed)
    random.seed(args.seed)
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)

    device = args.device
    datasets = [s.strip() for s in args.datasets.split(",") if s.strip()]
    extractors = [s.strip() for s in args.extractors.split(",") if s.strip()]
    targets = [s.strip() for s in args.targets.split(",") if s.strip()]

    # cross-target mapping
    # target gpt2 -> aux generator falcon
    # target falcon -> aux generator gpt2
    def aux_generator_for_target(tgt: str) -> str:
        if tgt == "gpt2":
            return "tiiuae/falcon-7b-instruct"
        if "falcon" in tgt.lower():
            return "gpt2"
        # fallback: use gpt2
        return "gpt2"

    # Prepare CSV
    with open(args.csv_out, "w", newline="", encoding="utf-8") as fcsv:
        writer = csv.DictWriter(
            fcsv,
            fieldnames=[
                "dataset", "target_model", "aux_generator", "extractor",
                "n_fit", "n_eval", "sigma",
                "accuracy", "auc", "tpr_at_1fpr", "roc_png"
            ]
        )
        writer.writeheader()

        # For efficiency: generate aux texts per (dataset, aux_generator) once and reuse across extractors
        aux_text_cache = {}  # (dataset, aux_gen) -> list[str]

        for ds_name in datasets:
            mem_fit, mem_eval = load_real_member_pool(ds_name, args.n_fit, args.n_eval, args.seed)
            print(f"\n[DATA] {ds_name}: mem_fit={len(mem_fit)} mem_eval={len(mem_eval)}", flush=True)

            for tgt in targets:
                aux_gen = aux_generator_for_target(tgt)
                print(f"\n[TARGET] {tgt}  -> AUX GEN: {aux_gen}", flush=True)

                # get / build aux texts (size = n_eval, because eval negatives should match)
                key = (ds_name, aux_gen, args.n_eval, args.aux_max_new_tokens, args.seed)
                if key not in aux_text_cache:
                    aux_text_cache[key] = generate_aux_texts(
                        gen_name=aux_gen,
                        dataset=ds_name,
                        n_aux=args.n_eval,
                        device=device,
                        max_new_tokens=args.aux_max_new_tokens,
                        temperature=args.aux_temperature,
                        top_p=args.aux_top_p,
                        seed=args.seed,
                        cache_dir=args.aux_cache_dir,
                    )
                aux_eval = aux_text_cache[key]
                assert len(aux_eval) == args.n_eval, f"aux_eval size mismatch: {len(aux_eval)} vs {args.n_eval}"

                for ext in extractors:
                    print(f"\n[RUN] ds={ds_name} target={tgt} aux={aux_gen} extractor={ext} sigma={args.sigma}", flush=True)

                    enc_tok = AutoTokenizer.from_pretrained(ext)
                    enc_model = AutoModel.from_pretrained(ext).to(device).eval()

                    # Encode noisy embeddings
                    X_fit = encode_texts_noisy(mem_fit, enc_tok, enc_model, device, args.enc_bs, args.enc_trunc_len, args.sigma)
                    X_mem = encode_texts_noisy(mem_eval, enc_tok, enc_model, device, args.enc_bs, args.enc_trunc_len, args.sigma)
                    X_aux = encode_texts_noisy(aux_eval, enc_tok, enc_model, device, args.enc_bs, args.enc_trunc_len, args.sigma)

                    # Centers
                    mu_real = X_fit.mean(axis=0)
                    mu_real = mu_real / (np.linalg.norm(mu_real) + 1e-9)

                    mu_aux = X_aux.mean(axis=0)
                    mu_aux = mu_aux / (np.linalg.norm(mu_aux) + 1e-9)

                    # Scores
                    s_mem = cosine_with_center(X_mem, mu_real) - cosine_with_center(X_mem, mu_aux)
                    s_aux = cosine_with_center(X_aux, mu_real) - cosine_with_center(X_aux, mu_aux)

                    y_true = np.array([1]*len(s_mem) + [0]*len(s_aux))
                    y_score = np.concatenate([s_mem, s_aux])

                    auc = float(roc_auc_score(y_true, y_score))
                    fpr, tpr, _ = roc_curve(y_true, y_score)

                    # flip if inverted
                    if auc < 0.5:
                        y_score = -y_score
                        auc = float(roc_auc_score(y_true, y_score))
                        fpr, tpr, _ = roc_curve(y_true, y_score)

                    tpr1 = compute_tpr_at_fpr(fpr, tpr, 0.01)
                    acc = float(((y_score > 0).astype(int) == y_true).mean())

                    roc_png = f"roc_{sanitize(ds_name)}_{sanitize(tgt)}_{sanitize(ext)}.png"
                    plt.figure(figsize=(5, 5))
                    plt.plot(fpr, tpr, label=f"AUC={auc:.3f}")
                    plt.plot([0, 1], [0, 1], "k--")
                    plt.scatter([0.01], [tpr1], label=f"TPR@1%FPR={tpr1:.3f}")
                    plt.xlabel("False Positive Rate")
                    plt.ylabel("True Positive Rate")
                    plt.title(f"Cross-target Noise-only Cosine MIA\n{ds_name} | target={tgt}\naux={aux_gen} | ext={ext} | sigma={args.sigma}")
                    plt.legend(loc="lower right")
                    plt.tight_layout()
                    plt.savefig(roc_png, dpi=150)
                    plt.close()

                    writer.writerow({
                        "dataset": ds_name,
                        "target_model": tgt,
                        "aux_generator": aux_gen,
                        "extractor": ext,
                        "n_fit": args.n_fit,
                        "n_eval": args.n_eval,
                        "sigma": round(float(args.sigma), 6),
                        "accuracy": round(acc, 6),
                        "auc": round(auc, 6),
                        "tpr_at_1fpr": round(float(tpr1), 6),
                        "roc_png": roc_png,
                    })
                    fcsv.flush()

                    # cleanup
                    del enc_tok, enc_model
                    torch.cuda.empty_cache()

                    print(f"[RESULT] acc={acc:.4f} auc={auc:.4f} tpr@1%fpr={tpr1:.4f} -> {roc_png}", flush=True)

    print(f"\nAll done. CSV saved: {args.csv_out}", flush=True)


if __name__ == "__main__":
    main()

